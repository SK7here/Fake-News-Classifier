{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To work with nd-arrays\n",
    "import numpy as np\n",
    "\n",
    "#To work with data structures\n",
    "import pandas as pd\n",
    "\n",
    "#To plot graphs within terminal(for Jupyter Notebooks only)\n",
    "%matplotlib inline\n",
    "\n",
    "#To compute accuracy for models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#To split dataset into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Importing Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Preprocessing Text documents(articles)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#To build Convolution Neural Network\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "#To plot graphs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining constants for easy usage\n",
    "MAX_SEQUENCE_LENGTH = 5000\n",
    "MAX_NUM_WORDS = 25000\n",
    "TEST_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "TEXT_DATA = 'data/fake_or_real_news.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function defined to evaluate models\n",
    "def evaluate_model(pred_func, X_train, y_train, X_test, y_test):\n",
    "    #Training Accuracy\n",
    "    y_predict_train = pred_func(X_train)\n",
    "    train_acc = accuracy_score(y_train,y_predict_train)\n",
    "    \n",
    "    #Testing Accuracy\n",
    "    y_predict_test = pred_func(X_test)\n",
    "    test_acc = accuracy_score(y_test,y_predict_test)\n",
    "    \n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "df = pd.read_csv(TEXT_DATA)\n",
    "#Dropping variables that are irrelevant to our study\n",
    "df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
    "#Filtering out articles with no text\n",
    "mask = list(df['text'].apply(lambda x: len(x) > 0))\n",
    "df = df[mask]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring all text samples have their labels\n",
    "texts = df['text']\n",
    "labels = df['label']\n",
    "\n",
    "print('Found %s texts.' %texts.shape)\n",
    "print('Found %s labels.' %labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding word count of each article\n",
    "text_lengths = texts.apply(lambda x: len(x.split(\" \")))\n",
    "print(\"\\nMaximum number of words in an article\")\n",
    "print(text_lengths.max())\n",
    "\n",
    "#Histogram plot for word count\n",
    "print(\"\\nHistogram plot for articles upto 5000 words\")\n",
    "plt.hist(text_lengths, bins=[0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000])\n",
    "plt.ylabel(\"Article count\")\n",
    "plt.xlabel(\"No of words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method I - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vector models for training and testing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# data vectorizer\n",
    "    #max_df/min_df = int->no of documents ; float->percentage among total documents\n",
    "    #stop_words = english-> inbulit stop words list for english is used\n",
    "    #binary = True-> if a word occurs even once, assigns '1'\n",
    "    #analyzer = features are taken as words    \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             binary = True, \n",
    "                             min_df = 2,\n",
    "                             stop_words='english')\n",
    "#Vectorizer is fit for the dataset\n",
    "docarray = vectorizer.fit_transform(texts).toarray()\n",
    "#Displaying output of Count Vectorization as a dataframe\n",
    "    #vectorizer.vocabulary_ -> returns feature names(words)\n",
    "docterm = pd.DataFrame(docarray, columns=vectorizer.vocabulary_)\n",
    "print(\"\\nAfter Count Vectorization\\n\")\n",
    "print(docterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test data\n",
    "    #One hot encoding the categorical dependent labels\n",
    "docterm_train, docterm_test, y_train, y_test = train_test_split(docterm, labels.apply(lambda x: 0 if x == 'FAKE' else 1), test_size=TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Multinomial Naive Bayes model\n",
    "    #Calculates probability of a word occuring in each class(FAKE/REAL) based on given input\n",
    "    #Considers each word as an independent feature\n",
    "model = MultinomialNB()\n",
    "model.fit(docterm_train, y_train)\n",
    "\n",
    "#Computing Training and validation accuracy\n",
    "train_acc, test_acc = evaluate_model(model.predict, docterm_train, y_train, docterm_test, y_test)\n",
    "print(\"Training Accuracy: {:.2f}\".format(train_acc*100))\n",
    "print(\"Testing Accuracy: {:.2f}\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method II - Convolutional DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To keep only \"MAX_NUM_WORDS - 25000\" most common words\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "#Creating Vocabulary index based on word frequency; lower the index, higher the frequency\n",
    "tokenizer.fit_on_texts(texts)\n",
    "#Replacing words with corresponding word index taken from fit_on_texts\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"\\nUnique words found in the dataset are listed below arranged according to most occurence frequency\\n\")\n",
    "print(word_index)\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "\n",
    "'''To make each sequence in list to have \"MAX_SEQUENCE_LENGTH - 5000\" values by padding 0's in front of each sequence \n",
    "and truncating words in front if sequence has over 5000 values'''\n",
    "    \n",
    "data = pad_sequences(sequences, \n",
    "                     maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding='pre', \n",
    "                     truncating='pre')\n",
    "\n",
    "print('Found {} unique tokens.' .format(len(word_index)))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test data\n",
    "    #One hot encoding the categorical dependent labels\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, \n",
    "                                                  labels.apply(lambda x: 0 if x == 'FAKE' else 1), \n",
    "                                                  test_size=TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the CNN model\n",
    "model = Sequential(\n",
    "    [\n",
    "        #representing text as continous vector represenrations and perform word embeddings - find similar words\n",
    "        layers.Embedding(num_words, #Size of vocabulary\n",
    "                         EMBEDDING_DIM,\n",
    "                         input_length = MAX_SEQUENCE_LENGTH, #Length of each sequence(article)\n",
    "                         trainable=True), #To update weights during training)\n",
    "        \n",
    "        #Conv1D useful for NLP\n",
    "        #number of output filters = 128 \n",
    "        #window size = 5; 5 words are considered at a time\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        \n",
    "        #Pooling done to reduce spatial size of representation and reduce computations in neural networks\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        \n",
    "        #Forming a fully connected hidden layer\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        #Forming a fully connected output layer\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', #restricts oscillations\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "history = model.fit(x_train, \n",
    "                    y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "train_acc, test_acc = evaluate_model(model.predict(),\n",
    "                                     x_train, \n",
    "                                     y_train, \n",
    "                                     x_val, \n",
    "                                     y_val)\n",
    "print(\"Training Accuracy: {:.2f}\".format(train_acc*100))\n",
    "print(\"Testing Accuracy: {:.2f}\".format(test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
